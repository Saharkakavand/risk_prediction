{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "import rnn_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_recurrent_cell(hidden_dim, dropout_keep_prob):\n",
    "    return rnn_cells.LayerNormLSTMCell(\n",
    "        hidden_dim, \n",
    "        use_recurrent_dropout=True,\n",
    "        dropout_keep_prob=dropout_keep_prob\n",
    "    )\n",
    "\n",
    "def compute_n_batches(n_samples, batch_size):\n",
    "    n_batches = n_samples // batch_size\n",
    "    if n_samples % batch_size != 0:\n",
    "        n_batches += 1\n",
    "    return n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    pass\n",
    "\n",
    "class RNN(Network):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            name,\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            max_len,\n",
    "            output_dim,\n",
    "            batch_size=100,\n",
    "            dropout_keep_prob=1.,\n",
    "            learning_rate=0.001,\n",
    "            grad_clip=1.):\n",
    "        self.name = name\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_len = max_len\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.build_placeholders()\n",
    "            self.build_network()\n",
    "            self.build_loss()\n",
    "            self.build_train_op()\n",
    "            self.build_summaries()\n",
    "    \n",
    "    def build_placeholders(self):\n",
    "        self.inputs = tf.placeholder(tf.float32, (self.batch_size, self.max_len, self.input_dim), 'inputs')\n",
    "        self.targets = tf.placeholder(tf.int32, (self.batch_size, self.max_len), 'targets')\n",
    "        self.lengths = tf.placeholder(tf.int32, (self.batch_size,), 'lengths')\n",
    "        self.sequence_mask = tf.sequence_mask(self.lengths, maxlen=self.max_len, dtype=tf.float32)\n",
    "        self.dropout_keep_prop_ph = tf.placeholder_with_default(self.dropout_keep_prob, (), 'dropout_keep_prob')\n",
    "        \n",
    "    def build_network(self):\n",
    "        self.cell_fw = _build_recurrent_cell(self.hidden_dim, self.dropout_keep_prop_ph)\n",
    "        self.cell_bw = _build_recurrent_cell(self.hidden_dim, self.dropout_keep_prop_ph)\n",
    "        \n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            self.cell_fw,\n",
    "            self.cell_bw,\n",
    "            inputs=self.inputs,\n",
    "            sequence_length=self.lengths,\n",
    "            dtype=tf.float32,\n",
    "            time_major=False\n",
    "        )\n",
    "        \n",
    "        outputs = tf.concat(outputs, axis=1)\n",
    "        outputs = tf.reshape(outputs, (self.batch_size * self.max_len, -1))\n",
    "        scores = tf.contrib.layers.fully_connected(\n",
    "            outputs,\n",
    "            self.output_dim,\n",
    "            activation_fn=None\n",
    "        )\n",
    "        self.scores = tf.reshape(scores, (self.batch_size, self.max_len, self.output_dim))\n",
    "        self.probs = tf.nn.softmax(self.scores)\n",
    "        \n",
    "    def build_loss(self):\n",
    "        \n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.targets, logits=self.scores)\n",
    "        loss = tf.reduce_sum(self.sequence_mask * losses, axis=1) / tf.cast(self.lengths, tf.float32)\n",
    "        self.loss = tf.reduce_mean(loss)\n",
    "        \n",
    "    def build_train_op(self):\n",
    "        self.var_list = tf.trainable_variables()\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        grads_vars = optimizer.compute_gradients(self.loss, self.var_list)\n",
    "        clipped_grads_vars = [(tf.clip_by_value(g, -self.grad_clip, self.grad_clip), v) for (g,v) in grads_vars]\n",
    "        self.train_op = optimizer.apply_gradients(clipped_grads_vars)\n",
    "        \n",
    "    def build_summaries(self):\n",
    "        pass\n",
    "        \n",
    "    def train(self, data, n_epochs=100):\n",
    "        sess = tf.get_default_session()\n",
    "        \n",
    "        n_samples = len(data['train_x'])\n",
    "        n_batches = compute_n_batches(n_samples, self.batch_size)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            total_loss = 0\n",
    "            for bidx in range(n_batches):\n",
    "                s = bidx * self.batch_size\n",
    "                e = s + self.batch_size\n",
    "                \n",
    "                feed_dict = {\n",
    "                    self.inputs:data['train_x'][s:e],\n",
    "                    self.targets:data['train_y'][s:e],\n",
    "                    self.lengths:data['train_lengths'][s:e]\n",
    "                }\n",
    "                outputs_list = [self.loss, self.train_op]\n",
    "                loss, _ = sess.run(outputs_list, feed_dict=feed_dict)\n",
    "                total_loss += loss\n",
    "                sys.stdout.write('\\repoch: {} / {} batch: {} / {} loss: {}'.format(\n",
    "                    epoch+1, n_epochs, bidx+1, n_batches, total_loss / (self.batch_size * (bidx+1))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "input_dim = 4\n",
    "hidden_dim = 16\n",
    "max_len = 8\n",
    "output_dim = 2\n",
    "batch_size = 100\n",
    "model = RNN(\n",
    "    'test', \n",
    "    input_dim, \n",
    "    hidden_dim, \n",
    "    max_len, \n",
    "    output_dim, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = batch_size\n",
    "x = np.random.randn(n_samples, max_len, input_dim)\n",
    "y = np.sum(x, axis=(2))\n",
    "y[y>0] = 1\n",
    "y[y<=0] = 0\n",
    "y = y.astype(int)\n",
    "data = dict(\n",
    "    train_x=x, \n",
    "    train_y=y, \n",
    "    train_lengths=np.ones(n_samples, dtype=int) * max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 / 100 batch: 1 / 1 loss: 5.4935552179813384e-05"
     ]
    }
   ],
   "source": [
    "model.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:riskenv]",
   "language": "python",
   "name": "conda-env-riskenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
