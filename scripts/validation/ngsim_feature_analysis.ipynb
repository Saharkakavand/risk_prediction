{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '../../data/datasets/ngsim_feature_trajectories.h5'\n",
    "infile = h5py.File(filepath, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.copy(infile['1'].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = infile.attrs['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_lengths(arr):\n",
    "    sums = np.sum(np.array(arr), axis=2)\n",
    "    lengths = []\n",
    "    for sample in sums:\n",
    "        zero_idxs = np.where(sample == 0.)[0]\n",
    "        if len(zero_idxs) == 0:\n",
    "            lengths.append(len(sample))\n",
    "        else:\n",
    "            lengths.append(zero_idxs[0])\n",
    "    return lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lengths = compute_lengths(x)\n",
    "\n",
    "fidx = 18\n",
    "censor = 100.\n",
    "feats = []\n",
    "for (i,l) in enumerate(lengths):\n",
    "    cur_feats = np.copy(x[i,:l,fidx])\n",
    "\n",
    "#     valid_idxs = np.where(cur_feats != censor)[0]\n",
    "#     invalid_idxs = set(np.where(cur_feats == censor)[0])\n",
    "    \n",
    "#     for (prev, cur) in zip(valid_idxs, valid_idxs[1:]):\n",
    "#         if cur - prev == 2:\n",
    "#             idx = (cur + prev) // 2\n",
    "#             if idx in invalid_idxs:\n",
    "#                 val = (cur_feats[prev] + cur_feats[cur]) / 2\n",
    "#                 cur_feats[idx] = val\n",
    "                \n",
    "#     for j in range(1, l-1):\n",
    "\n",
    "#         if cur_feats[j] == censor and cur_feats[j-1] != censor and cur_feats[j+1] != censor:\n",
    "#             cur_feats[j] = (cur_feats[j-1] + cur_feats[j+1]) / 2\n",
    "            \n",
    "#         if cur_feats[j] == censor and x[i,j,fidx+1] != censor and x[i,j,fidx-1] != censor:\n",
    "#             cur_feats[j] = (x[i,j,fidx+1] + x[i,j,fidx-1]) / 2\n",
    "        \n",
    "#     invalid_idxs = set(np.where(cur_feats == censor)[0])\n",
    "#     if len(invalid_idxs) / l < .2:\n",
    "    feats.append(cur_feats)\n",
    "    \n",
    "    \n",
    "    \n",
    "z = np.concatenate(feats)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(z,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_lengths(arr):\n",
    "    sums = np.sum(np.array(arr), axis=2)\n",
    "    lengths = []\n",
    "    for sample in sums:\n",
    "        zero_idxs = np.where(sample == 0.)[0]\n",
    "        if len(zero_idxs) == 0:\n",
    "            lengths.append(len(sample))\n",
    "        else:\n",
    "            lengths.append(zero_idxs[0])\n",
    "    return lengths\n",
    "\n",
    "def load_ngsim_trajectory_data(\n",
    "        filepath, \n",
    "        traj_key=None,\n",
    "        feature_keys=[\n",
    "            'velocity',\n",
    "            'relative_offset',\n",
    "            'relative_heading',\n",
    "            'length',\n",
    "            'width',\n",
    "            'lane_curvature',\n",
    "            'markerdist_left',\n",
    "            'markerdist_right',\n",
    "            'accel',\n",
    "            'jerk',\n",
    "            'turn_rate_frenet',\n",
    "            'angular_rate_frenet'\n",
    "        ],\n",
    "        target_keys=[\n",
    "            'lidar_10'\n",
    "        ],\n",
    "        binedges=[10,15,25,50],\n",
    "        max_censor_ratio=.3,\n",
    "        max_len=None,\n",
    "        train_ratio=.9,\n",
    "        max_samples=None,\n",
    "        shuffle=True,\n",
    "        normalize=True,\n",
    "        censor=50.):\n",
    "    # select from the different roadways\n",
    "    infile = h5py.File(filepath, 'r')\n",
    "    if traj_key is None:\n",
    "        x = np.vstack([infile[k].value for k in infile.keys()])\n",
    "    else:\n",
    "        x = np.copy(infile[traj_key].value)\n",
    "\n",
    "    # enforce max_len\n",
    "    if max_len is not None:\n",
    "        x = x[:,:max_len,:]\n",
    "    \n",
    "    # pandas format for feature-based selection\n",
    "    panel = pd.Panel(\n",
    "        data=x, \n",
    "        minor_axis=infile.attrs['feature_names']\n",
    "    )\n",
    "    \n",
    "    lengths = np.array(compute_lengths(panel[:,:,feature_keys]))\n",
    "    n_samples, n_timesteps, input_dim = panel[:,:,feature_keys].shape\n",
    "\n",
    "    # only a single target key implemented for now\n",
    "    assert len(target_keys) == 1\n",
    "    k = target_keys[0]\n",
    "        \n",
    "    # remove samples with too many censored values\n",
    "    valid_sample_idxs = []\n",
    "    for j, l in enumerate(lengths):\n",
    "        invalid_idxs = np.where(panel[j,:l,k] == censor)[0]\n",
    "        if len(invalid_idxs) / l < max_censor_ratio:\n",
    "            valid_sample_idxs.append(j)\n",
    "    valid_sample_idxs = np.array(valid_sample_idxs)\n",
    "            \n",
    "    # debugging size\n",
    "    if max_samples is not None:\n",
    "        valid_sample_idxs = valid_sample_idxs[:max_samples]\n",
    "    \n",
    "    # shuffle\n",
    "    if shuffle:\n",
    "        permute_idxs = np.random.permutation(len(valid_sample_idxs))\n",
    "        valid_sample_idxs = valid_sample_idxs[permute_idxs]\n",
    "\n",
    "    y = np.zeros((len(valid_sample_idxs), n_timesteps), dtype=int)\n",
    "    lengths = lengths[valid_sample_idxs]\n",
    "    x = np.array(panel[valid_sample_idxs,:,feature_keys])\n",
    "    \n",
    "    # discretize the targets\n",
    "    y[:,:] = np.digitize(\n",
    "        panel[valid_sample_idxs,:,k].T, \n",
    "        binedges, \n",
    "        right=True\n",
    "    )\n",
    "        \n",
    "    # normalize features\n",
    "    if normalize:\n",
    "        x -= np.mean(x, axis=(1,2), keepdims=True)\n",
    "        x /= np.std(x, axis=(1,2), keepdims=True) + 1e-8\n",
    "    \n",
    "    # train / val split\n",
    "    train_idx = int(len(valid_sample_idxs) * train_ratio)\n",
    "    train_x = x[:train_idx]\n",
    "    train_y = y[:train_idx]\n",
    "    train_lengths = lengths[:train_idx]\n",
    "    val_x = x[train_idx:]\n",
    "    val_y = y[train_idx:]\n",
    "    val_lengths = lengths[train_idx:]\n",
    "    \n",
    "    data = dict(\n",
    "        train_x=train_x,\n",
    "        train_y=train_y,\n",
    "        train_lengths=train_lengths,\n",
    "        val_x=val_x,\n",
    "        val_y=val_y,\n",
    "        val_lengths=val_lengths,\n",
    "        feature_names=feature_keys,\n",
    "        target_names=target_keys,\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_1 = load_ngsim_trajectory_data(filepath, target_keys=['lidar_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_5 = load_ngsim_trajectory_data(filepath, target_keys=['lidar_5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_10 = load_ngsim_trajectory_data(filepath, target_keys=['lidar_10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_changes(arr):\n",
    "    total, count = 0, 0\n",
    "    for row in arr:\n",
    "        for prev, cur in zip(row, row[1:]):\n",
    "            total += 1\n",
    "            if prev != cur:\n",
    "                count += 1\n",
    "    return count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(compute_changes(data_1['train_y']))\n",
    "print(compute_changes(data_5['train_y']))\n",
    "print(compute_changes(data_10['train_y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(data['train_y']))\n",
    "print(len(data['train_x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('i: {} #: {}'.format(i, len(np.where(data['train_y'] == i)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.size(data['train_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "10563 / 239238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "same = tf.equal(tf.cast(np.argmax(self.scores, axis=-1), tf.int32), self.targets)\n",
    "        same = tf.cast(same, tf.float32) * self.sequence_mask\n",
    "        self.acc = tf.reduce_sum(same) / tf.reduce_sum(tf.cast(self.lengths, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "scores = np.array([[1,2],[2,1],[1,2]])\n",
    "targets = np.array([1,0,0])\n",
    "same = tf.equal(tf.cast(np.argmax(scores, axis=-1), tf.int32), self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = [0,1,2]\n",
    "p = [0,0,0]\n",
    "print(sklearn.metrics.precision_recall_fscore_support(t, p, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(range(2,5))+ [1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(np.random.randint(low=0, high=5, size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_batch_idxs(start, batch_size, size):\n",
    "    if start >= size:\n",
    "        return list(np.random.randint(low=0, high=size, size=batch_size))\n",
    "    \n",
    "    end = start + batch_size\n",
    "\n",
    "    if end <= size:\n",
    "        return list(range(start, end))\n",
    "\n",
    "    else:\n",
    "        base_idxs = list(range(start, size))\n",
    "        remainder = end - size\n",
    "        idxs = list(np.random.randint(low=0, high=size, size=remainder))\n",
    "        return base_idxs + idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_batch_idxs(7, 1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.randint(0,2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctr = collections.Counter([1,2,3,4,4])\n",
    "ctr.most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.16766467,  0.22580645,  0.17708333,  0.21296296,  0.19230769]),\n",
       " array([ 0.15384615,  0.24019608,  0.17708333,  0.2081448 ,  0.19900498]),\n",
       " array([ 0.16045845,  0.2327791 ,  0.17708333,  0.21052632,  0.19559902]),\n",
       " array([182, 204, 192, 221, 201]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.dummy\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "c = sklearn.dummy.DummyClassifier('stratified')\n",
    "nclasses = 5\n",
    "targets = np.random.randint(0,nclasses,size=1000)\n",
    "c.fit(None, targets)\n",
    "preds = c.predict(targets.reshape(-1,1))\n",
    "sklearn.metrics.precision_recall_fscore_support(targets, preds, average=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:riskenv]",
   "language": "python",
   "name": "conda-env-riskenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
